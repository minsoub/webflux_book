# Chapter 19. 성능 최적화

리액티브 아키텍처를 채택했다고 자동으로 높은 성능이 따라오는 건 아니다. 실제로 논블로킹 모델의 이점을 제대로 누리려면, 병목 지점을 정확히 측정하고, 커넥션 풀과 이벤트 루프를 우리 애플리케이션 특성에 맞게 조정해야 한다. 여기에 캐싱으로 불필요한 I/O를 줄이고, 블로킹 코드를 철저히 제거해야 진정한 고성능을 얻을 수 있다. 이번 장에서는 리액티브 애플리케이션의 **성능 측정 방법**부터 **MongoDB 커넥션 풀 튜닝**, **Netty 이벤트 루프 최적화**, **캐싱 전략**, **BlockHound를 활용한 블로킹 탐지**, 그리고 **Gatling/k6를 활용한 부하 테스트**까지 실전 성능 최적화의 전 과정을 살펴본다.

---

## 19.1 리액티브 애플리케이션 성능 측정

성능 최적화의 첫 번째 원칙은 간단하다: **측정 없이 최적화하지 말 것**. 감에 의존한 최적화는 코드 복잡도만 높일 뿐, 실질적인 개선을 가져오지 않는다.

### 19.1.1 핵심 성능 지표

성능을 이야기할 때 보통 세 가지 축으로 나눠서 본다.

| 지표 | 설명 | 측정 단위 |
|------|------|----------|
| **처리량(Throughput)** | 단위 시간당 처리한 요청 수 | req/sec |
| **지연시간(Latency)** | 요청 시작부터 응답 완료까지 소요 시간 | ms (p50, p95, p99) |
| **리소스 사용률** | CPU, 메모리, 스레드, 커넥션 점유율 | %, 개수 |

리액티브 애플리케이션의 핵심은 적은 스레드로 높은 처리량을 달성하는 것이다. 그래서 스레드 수 대비 처리량의 비율이 정말 중요한 평가 기준이 된다. 지연시간을 볼 때도 단순 평균은 거의 무의미하다. p95, p99 같은 백분위를 봐야 실제 사용자가 경험하는 성능을 알 수 있다.

```
[전통적 MVC 모델]
스레드 200개 → 동시 처리 200 요청 → 처리량 ~2,000 req/sec

[리액티브 모델]
스레드 8개(이벤트 루프) → 동시 처리 수천 요청 → 처리량 ~10,000+ req/sec
```

### 19.1.2 Micrometer 메트릭 활용

이전 장에서 설정한 Micrometer를 이제 성능 분석에 적극 활용해야 한다. WebFlux가 자동으로 수집하는 핵심 메트릭들을 보자.

| 메트릭 이름 | 설명 |
|------------|------|
| `http.server.requests` | HTTP 요청 처리 시간 (타이머) |
| `reactor.netty.http.server.data.received` | 서버가 수신한 데이터 바이트 |
| `mongodb.driver.pool.size` | MongoDB 커넥션 풀 크기 |
| `mongodb.driver.pool.waitqueuesize` | MongoDB 커넥션 대기 큐 크기 |
| `jvm.threads.live` | 활성 JVM 스레드 수 |

이런 기본 메트릭 외에 커스텀 메트릭을 추가하면 비즈니스 로직의 성능도 측정할 수 있다.

```java
@Service
@RequiredArgsConstructor
public class ProductService {

    private final ProductRepository productRepository;
    private final MeterRegistry meterRegistry;

    public Mono<Product> findById(String id) {
        return Mono.defer(() -> {
            Timer.Sample sample = Timer.start(meterRegistry);

            return productRepository.findById(id)
                .doOnSuccess(p -> sample.stop(
                    Timer.builder("product.findById")
                        .tag("result", p != null ? "found" : "not_found")
                        .register(meterRegistry)
                ))
                .doOnError(e -> sample.stop(
                    Timer.builder("product.findById")
                        .tag("result", "error")
                        .register(meterRegistry)
                ));
        });
    }
}
```

### 19.1.3 JMH 마이크로벤치마크

JMH(Java Microbenchmark Harness)라는 도구를 알고 있나? JVM 수준의 정밀한 벤치마크를 할 때 필자도 자주 쓰는 도구인데, `build.gradle`에 플러그인을 추가하면 쉽게 시작할 수 있다.

```groovy
plugins {
    id 'me.champeau.jmh' version '0.7.2'
}

dependencies {
    jmh 'org.openjdk.jmh:jmh-core:1.37'
    jmh 'org.openjdk.jmh:jmh-generator-annprocess:1.37'
}
```

Reactor 연산자들의 성능 차이를 실제로 비교해보는 벤치마크를 작성해보자.

```java
@State(Scope.Thread)
@BenchmarkMode({Mode.Throughput, Mode.AverageTime})
@OutputTimeUnit(TimeUnit.MILLISECONDS)
public class ReactorBenchmark {

    private List<String> items;

    @Setup
    public void setup() {
        items = IntStream.range(0, 10_000)
            .mapToObj(i -> "item-" + i)
            .collect(Collectors.toList());
    }

    @Benchmark
    public void flatMap_동시성_기본값(Blackhole bh) {
        bh.consume(Flux.fromIterable(items)
            .flatMap(item -> Mono.fromCallable(() -> item.toUpperCase()))
            .collectList()
            .block());
    }

    @Benchmark
    public void map_단순변환(Blackhole bh) {
        bh.consume(Flux.fromIterable(items)
            .map(String::toUpperCase)
            .collectList()
            .block());
    }
}
```

> **참고**: `map`은 동기 변환이므로 `flatMap`보다 훨씬 빠르다. 비동기 I/O가 불필요한 단순 변환에는 항상 `map`을 사용해야 한다.

### 19.1.4 프로파일링 도구

프로파일링 도구도 여러 가지가 있으니 상황에 맞춰 고르면 된다.

| 도구 | 용도 | 특징 |
|------|------|------|
| **VisualVM** | CPU/메모리 프로파일링 | 무료, JDK 번들 |
| **async-profiler** | 저오버헤드 CPU/메모리 프로파일링 | 프로덕션 환경 사용 가능 |
| **JDK Flight Recorder (JFR)** | 포괄적 런타임 분석 | JDK 11+, 프로덕션 안전 |
| **IntelliJ Profiler** | IDE 통합 프로파일링 | 개발 시 편리 |

```bash
# async-profiler 실행 (애플리케이션 PID: 12345)
./asprof -d 30 -f profile.html -e cpu 12345

# JFR 기록 시작
java -XX:+FlightRecorder \
     -XX:StartFlightRecording=duration=60s,filename=recording.jfr \
     -jar application.jar
```

리액티브 애플리케이션의 프로파일링에서 꼭 봐야 할 부분이 하나 있다. 이벤트 루프 스레드(`reactor-http-nio-*`)의 CPU 사용률이 80%를 넘으면 병목 가능성이 매우 높다는 신호다. 필자의 경험상 이 지표가 80%를 넘으면 대부분 무거운 연산이 이벤트 루프에서 직접 실행되고 있었다. 이런 경우 별도 스케줄러로 그 작업을 오프로드하면 눈에 띄게 개선된다.

---

## 19.2 MongoDB 커넥션 풀 튜닝

MongoDB 드라이버가 내부적으로 커넥션 풀을 관리하는데, 이 풀의 설정이 전체 처리량에 미치는 영향은 정말 크다. 풀 크기, 타임아웃, 유휴 커넥션 관리 방식을 어떻게 설정하는지에 따라 성능이 크게 달라진다.

### 19.2.1 기본 커넥션 풀 동작

MongoDB Reactive Streams 드라이버의 커넥션 풀 기본값은 다음과 같다.

| 설정 | 기본값 | 설명 |
|------|--------|------|
| `minPoolSize` | 0 | 최소 유지 커넥션 수 |
| `maxPoolSize` | 100 | 최대 커넥션 수 |
| `maxWaitTime` | 120초 | 커넥션 획득 대기 시간 |
| `maxConnectionIdleTime` | 0 (무제한) | 유휴 커넥션 유지 시간 |
| `maxConnectionLifeTime` | 0 (무제한) | 커넥션 최대 수명 |

### 19.2.2 MongoClientSettings를 활용한 커넥션 풀 설정

`application.yml`에 URI 파라미터로 설정하는 방법도 있지만, `MongoClientSettings` 빈을 직접 구성하면 훨씬 세밀한 제어가 가능하다. 이게 실무에서는 훨씬 흔한 방식이다.

```java
@Configuration
public class MongoConfig extends AbstractReactiveMongoConfiguration {

    @Value("${spring.data.mongodb.uri}")
    private String mongoUri;

    @Override
    protected String getDatabaseName() {
        return "myapp";
    }

    @Override
    @Bean
    public MongoClient reactiveMongoClient() {
        ConnectionString connString = new ConnectionString(mongoUri);

        MongoClientSettings settings = MongoClientSettings.builder()
            .applyConnectionString(connString)
            .applyToConnectionPoolSettings(pool -> pool
                .minSize(10)                                     // 최소 커넥션
                .maxSize(50)                                     // 최대 커넥션
                .maxWaitTime(5, TimeUnit.SECONDS)                // 커넥션 대기 타임아웃
                .maxConnectionIdleTime(30, TimeUnit.SECONDS)     // 유휴 커넥션 정리
                .maxConnectionLifeTime(5, TimeUnit.MINUTES)      // 커넥션 최대 수명
                .maintenanceFrequency(30, TimeUnit.SECONDS)      // 정리 주기
            )
            .applyToSocketSettings(socket -> socket
                .connectTimeout(3, TimeUnit.SECONDS)
                .readTimeout(10, TimeUnit.SECONDS)
            )
            .applyToServerSettings(server -> server
                .heartbeatFrequency(10, TimeUnit.SECONDS)
                .minHeartbeatFrequency(500, TimeUnit.MILLISECONDS)
            )
            .build();

        return MongoClients.create(settings);
    }
}
```

### 19.2.3 풀 크기 산정 가이드라인

풀 크기를 정할 때는 다음 공식을 기준으로 생각하면 된다.

```
최적 풀 크기 = (동시 요청 수) x (평균 쿼리 시간) / (목표 응답 시간)
```

예시를 들어보자. 동시 요청이 500건이고, 평균 쿼리 시간이 10ms, 목표 응답 시간이 100ms라면? 계산하면 `500 x 10 / 100 = 50`이 나온다. 이게 적정 풀 크기다.

| 시나리오 | minSize | maxSize | 근거 |
|----------|---------|---------|------|
| **개발 환경** | 2 | 10 | 리소스 절약 |
| **소규모 서비스** | 5 | 30 | 동시 사용자 ~100명 |
| **중규모 서비스** | 10 | 50 | 동시 사용자 ~1,000명 |
| **대규모 서비스** | 20 | 100 | 동시 사용자 ~10,000명 |

> **주의**: `maxSize`를 무조건 크게 잡으면 MongoDB 서버 측 리소스가 고갈될 수 있다. 모든 애플리케이션 인스턴스의 `maxSize` 합이 MongoDB의 `net.maxIncomingConnections`(기본 65,536)의 80%를 넘지 않도록 한다.

### 19.2.4 커넥션 풀 모니터링과 타임아웃

이제 실제로 커넥션 풀이 잘 동작하는지 모니터링해야 한다. 메트릭을 활성화하고, 타임아웃은 계층별로 정리해서 설정해보자.

```yaml
# application.yml
management:
  metrics:
    mongo:
      connectionpool:
        enabled: true
      command:
        enabled: true
```

```
# Prometheus 메트릭 예시
mongodb_driver_pool_size{server_address="localhost:27017"} 25
mongodb_driver_pool_checkedout{server_address="localhost:27017"} 12
mongodb_driver_pool_waitqueuesize{server_address="localhost:27017"} 0
```

`waitqueuesize`가 계속 0보다 크다는 건 위험한 신호다. `maxSize`를 늘리거나, 아니면 쿼리 자체를 더 빠르게 만들어야 한다는 뜻이다. 타임아웃은 다음처럼 계층별로 구성하는 게 좋다.

```
소켓 타임아웃 (connectTimeout, readTimeout)
  └─ 쿼리 타임아웃 (maxTime)
       └─ Reactor 타임아웃 (timeout 연산자)
            └─ HTTP 응답 타임아웃 (WebFlux 타임아웃)
```

```java
public Flux<Product> findByCategory(String category) {
    Query query = new Query(Criteria.where("category").is(category))
        .maxTime(Duration.ofSeconds(5));  // 쿼리 레벨 타임아웃 (5초)

    return mongoTemplate.find(query, Product.class)
        .timeout(Duration.ofSeconds(10));  // Reactor 레벨 타임아웃
}
```

---

## 19.3 Netty 이벤트 루프 최적화

Spring WebFlux는 내부적으로 Reactor Netty를 기본 서버로 쓰고 있다. Netty의 이벤트 루프를 어떻게 설정하는지가 전체 처리량을 좌우한다고 해도 과언이 아니다.

### 19.3.1 이벤트 루프 기본 구조

```
[Boss EventLoopGroup]       <- 새 커넥션 수락 (accept)
  └─ 스레드 1개 (보통)

[Worker EventLoopGroup]     <- I/O 이벤트 처리 (read/write)
  └─ 스레드 N개 (기본: CPU 코어 수)
    ├─ reactor-http-nio-1
    ├─ reactor-http-nio-2
    └─ reactor-http-nio-N
```

### 19.3.2 LoopResources를 활용한 이벤트 루프 커스터마이징

```java
@Configuration
public class NettyConfig {

    @Bean
    public NettyReactiveWebServerFactory nettyFactory() {
        NettyReactiveWebServerFactory factory = new NettyReactiveWebServerFactory();

        factory.addServerCustomizers(httpServer -> {
            LoopResources loopResources = LoopResources.create(
                "custom-http",      // 스레드 이름 접두사
                1,                  // selector 스레드 수 (accept)
                Runtime.getRuntime().availableProcessors() * 2,  // worker 스레드 수
                true                // 데몬 스레드 여부
            );

            return httpServer
                .runOn(loopResources)
                .option(ChannelOption.SO_BACKLOG, 2048)          // 연결 대기 큐
                .childOption(ChannelOption.SO_KEEPALIVE, true)   // TCP Keep-Alive
                .childOption(ChannelOption.TCP_NODELAY, true);   // Nagle 알고리즘 비활성화
        });

        return factory;
    }
}
```

### 19.3.3 Native Transport (Epoll, KQueue)

JVM의 표준 NIO를 쓸 수도 있지만, 운영체제 수준의 네이티브 I/O를 사용하면 성능이 훨씬 좋아진다. 필자의 경험상 프로덕션 환경에서는 이 차이가 꽤 크게 느껴진다.

| Transport | 운영체제 | 장점 |
|-----------|---------|------|
| **NIO** (기본) | 모든 OS | 호환성 |
| **Epoll** | Linux | 낮은 지연시간, edge-triggered I/O |
| **KQueue** | macOS, BSD | macOS에서 최적 성능 |

```groovy
dependencies {
    // Linux Epoll
    runtimeOnly 'io.netty:netty-transport-native-epoll::linux-x86_64'

    // macOS KQueue (Intel / Apple Silicon)
    runtimeOnly 'io.netty:netty-transport-native-kqueue::osx-x86_64'
    runtimeOnly 'io.netty:netty-transport-native-kqueue::osx-aarch_64'
}
```

```java
@Configuration
public class NativeTransportConfig {

    @Bean
    public NettyReactiveWebServerFactory nettyFactory() {
        NettyReactiveWebServerFactory factory = new NettyReactiveWebServerFactory();

        factory.addServerCustomizers(httpServer -> {
            LoopResources loopResources = LoopResources.create(
                "native-http", 1,
                Runtime.getRuntime().availableProcessors(), true
            );
            return httpServer.runOn(loopResources, true);  // preferNative = true
        });

        return factory;
    }
}
```

`preferNative`를 `true`로 설정하면 Reactor Netty가 플랫폼에 맞는 네이티브 Transport를 자동으로 선택한다. 네이티브 라이브러리가 없으면 NIO로 자동 폴백한다.

### 19.3.4 이벤트 루프 블로킹 방지

이벤트 루프 스레드에서 블로킹 작업을 하면 안 된다. 이건 리액티브 프로그래밍의 대원칙이다. 무거운 연산은 별도 스케줄러로 넘기자.

```java
@Service
public class ReportService {

    // CPU 집약적 작업 전용 스케줄러
    private final Scheduler cpuScheduler = Schedulers.newParallel(
        "cpu-worker", Runtime.getRuntime().availableProcessors());

    // 레거시 블로킹 코드 전용 스케줄러
    private final Scheduler blockingScheduler = Schedulers.newBoundedElastic(
        100, 10_000, "blocking-worker", 60);

    public Mono<Report> generateReport(String reportId) {
        return loadData(reportId)
            .publishOn(cpuScheduler)         // CPU 작업은 별도 스레드에서
            .map(this::heavyComputation)
            .flatMap(this::saveReport);
    }

    public Mono<LegacyData> callLegacyApi(String id) {
        return Mono.fromCallable(() -> legacyService.getById(id))
            .subscribeOn(blockingScheduler);  // 블로킹 전용 스레드에서 실행
    }
}
```

---

## 19.4 캐싱 전략 (Caffeine, Redis)

I/O 연산을 줄이는 가장 확실한 방법은 뭘까? 그건 캐싱이다. 다만 리액티브 애플리케이션에서는 캐시 조회 자체도 논블로킹으로 이루어져야 한다는 점을 잊으면 안 된다.

### 19.4.1 Caffeine 로컬 캐시

Caffeine은 JVM 에서 쓸 수 있는 고성능 로컬 캐시 라이브러리다. 리액티브 환경에 맞게 래퍼 클래스를 한 번 만들어놓으면, 여러 곳에서 쉽게 재사용할 수 있다.

```groovy
dependencies {
    implementation 'com.github.ben-manes.caffeine:caffeine:3.1.8'
}
```

```java
@Component
public class ReactiveCaffeineCache<K, V> {

    private final Cache<K, V> cache;

    public ReactiveCaffeineCache(int maxSize, Duration ttl) {
        this.cache = Caffeine.newBuilder()
            .maximumSize(maxSize)
            .expireAfterWrite(ttl)
            .recordStats()
            .build();
    }

    public Mono<V> get(K key) {
        return Mono.justOrEmpty(cache.getIfPresent(key));
    }

    public Mono<V> get(K key, Function<K, Mono<V>> loader) {
        V cached = cache.getIfPresent(key);
        if (cached != null) {
            return Mono.just(cached);
        }
        return loader.apply(key)
            .doOnNext(value -> cache.put(key, value));
    }

    public Mono<Void> put(K key, V value) {
        cache.put(key, value);
        return Mono.empty();
    }

    public Mono<Void> evict(K key) {
        cache.invalidate(key);
        return Mono.empty();
    }
}
```

이제 이 캐시를 서비스에서 실제로 어떻게 써야 하는지 보자.

```java
@Service
public class ProductService {

    private final ProductRepository productRepository;
    private final ReactiveCaffeineCache<String, Product> productCache;

    public ProductService(ProductRepository productRepository) {
        this.productRepository = productRepository;
        this.productCache = new ReactiveCaffeineCache<>(10_000, Duration.ofMinutes(5));
    }

    public Mono<Product> findById(String id) {
        return productCache.get(id, key -> productRepository.findById(key));
    }

    public Mono<Product> update(String id, ProductUpdateRequest request) {
        return productRepository.findById(id)
            .flatMap(product -> {
                product.setName(request.getName());
                product.setPrice(request.getPrice());
                return productRepository.save(product);
            })
            .doOnNext(product -> productCache.put(id, product));
    }

    public Mono<Void> delete(String id) {
        return productRepository.deleteById(id)
            .then(productCache.evict(id));
    }
}
```

### 19.4.2 Reactive Redis 분산 캐시

애플리케이션이 여러 인스턴스로 운영되는 환경이면? 이때는 Reactive Redis를 분산 캐시로 써야 한다. 로컬 캐시만으로는 인스턴스 간에 데이터가 동기화되지 않기 때문이다.

```groovy
dependencies {
    implementation 'org.springframework.boot:spring-boot-starter-data-redis-reactive'
}
```

```yaml
spring:
  data:
    redis:
      host: localhost
      port: 6379
      timeout: 3s
      lettuce:
        pool:
          max-active: 50
          max-idle: 10
          min-idle: 5
```

```java
@Service
@RequiredArgsConstructor
public class RedisCacheService {

    private final ReactiveStringRedisTemplate redisTemplate;
    private final ObjectMapper objectMapper;

    public <T> Mono<T> getOrLoad(String key, Class<T> type,
                                  Duration ttl, Mono<T> loader) {
        return redisTemplate.opsForValue()
            .get(key)
            .flatMap(json -> deserialize(json, type))
            .switchIfEmpty(
                loader.flatMap(value ->
                    serialize(value)
                        .flatMap(json ->
                            redisTemplate.opsForValue()
                                .set(key, json, ttl)
                                .thenReturn(value)
                        )
                )
            );
    }

    public Mono<Boolean> evict(String key) {
        return redisTemplate.delete(key).map(count -> count > 0);
    }

    private <T> Mono<T> deserialize(String json, Class<T> type) {
        return Mono.fromCallable(() -> objectMapper.readValue(json, type))
            .onErrorResume(e -> Mono.empty());  // 역직렬화 실패 시 캐시 미스 처리
    }

    private <T> Mono<String> serialize(T value) {
        return Mono.fromCallable(() -> objectMapper.writeValueAsString(value));
    }
}
```

### 19.4.3 멀티 레벨 캐시 전략

로컬 캐시(Caffeine)와 분산 캐시(Redis)를 함께 쓰면 어떻게 될까? **L1/L2 캐시** 구조로 만들면 성능도 좋고 데이터 일관성도 유지할 수 있다.

```java
@Service
public class MultiLevelCacheService<T> {

    private final ReactiveCaffeineCache<String, T> l1Cache;   // 로컬
    private final RedisCacheService redisCacheService;         // 분산

    public MultiLevelCacheService(RedisCacheService redisCacheService) {
        this.l1Cache = new ReactiveCaffeineCache<>(5_000, Duration.ofMinutes(1));
        this.redisCacheService = redisCacheService;
    }

    public Mono<T> get(String key, Class<T> type,
                       Duration redisTtl, Mono<T> loader) {
        // L1 (Caffeine) -> L2 (Redis) -> 원본 데이터 소스
        return l1Cache.get(key)
            .switchIfEmpty(
                redisCacheService.getOrLoad(key, type, redisTtl, loader)
                    .doOnNext(value -> l1Cache.put(key, value))
            );
    }

    public Mono<Void> evict(String key) {
        return l1Cache.evict(key)
            .then(redisCacheService.evict(key))
            .then();
    }
}
```

```
요청 -> [L1 Caffeine 캐시] --히트--> 즉시 반환 (< 1ms)
              | 미스
              v
       [L2 Redis 캐시]    --히트--> L1에 저장 후 반환 (~1-3ms)
              | 미스
              v
       [MongoDB 조회]     -------> L1+L2에 저장 후 반환 (~5-50ms)
```

---

## 19.5 블로킹 코드 탐지 및 제거 (BlockHound)

리액티브 애플리케이션에서 가장 위험한 함정이 하나 있다. 바로 **이벤트 루프 스레드에서 몰래 일어나는 블로킹 호출**이다. 단 한 줄의 블로킹 코드도 전체 처리량을 극적으로 떨어뜨릴 수 있다. BlockHound는 이런 위험한 호출들을 런타임에 자동으로 발견해주는 도구다. Java Agent 방식으로 동작해서 코드 수정 없이도 탐지가 가능하다.

### 19.5.1 BlockHound 설정

```groovy
dependencies {
    testImplementation 'io.projectreactor.tools:blockhound:1.0.9.RELEASE'
}
```

```java
@SpringBootTest
class ApplicationBlockingTest {

    @BeforeAll
    static void setup() {
        BlockHound.install();
    }

    @Test
    void 블로킹_호출_없음_검증() {
        Mono.delay(Duration.ofMillis(1))
            .doOnNext(it -> {
                // 이벤트 루프 스레드에서 실행됨
                // 여기서 블로킹 호출이 있으면 예외 발생
            })
            .block();
    }
}
```

### 19.5.2 흔한 블로킹 코드 패턴과 수정

실무에서 자주 만나는 블로킹 코드들을 몇 가지 패턴으로 정리해봤다. 이 예시들을 알아두면 코드 리뷰할 때도 도움이 될 것 같다.

**패턴 1: 파일 I/O**

```java
// 블로킹 (위험)
public Mono<String> readFile(String path) {
    return Mono.just(Files.readString(Path.of(path)));  // 블로킹!
}

// 논블로킹 (수정)
public Mono<String> readFile(String path) {
    return Mono.fromCallable(() -> Files.readString(Path.of(path)))
        .subscribeOn(Schedulers.boundedElastic());
}
```

**패턴 2: Thread.sleep()**

```java
// 블로킹 (위험)
return Mono.fromCallable(() -> { Thread.sleep(1000); return "delayed"; });

// 논블로킹 (수정)
return Mono.delay(Duration.ofSeconds(1)).thenReturn("delayed");
```

**패턴 3: 동기 HTTP 호출**

```java
// 블로킹 (위험) - RestTemplate 사용
ExternalData data = restTemplate.getForObject(url, ExternalData.class);
return Mono.justOrEmpty(data);

// 논블로킹 (수정) - WebClient 사용
return webClient.get().uri(url).retrieve().bodyToMono(ExternalData.class);
```

**패턴 4: JDBC 호출**

```java
// 블로킹 (위험)
return Mono.justOrEmpty(jdbcTemplate.queryForObject(sql, mapper, id));

// 논블로킹 (수정 방법 1: boundedElastic으로 격리)
return Mono.fromCallable(() -> jdbcTemplate.queryForObject(sql, mapper, id))
    .subscribeOn(Schedulers.boundedElastic());

// 논블로킹 (수정 방법 2: R2DBC 사용 - Chapter 15 참조)
return r2dbcUserRepository.findById(id);
```

### 19.5.3 BlockHound 커스텀 설정과 테스트 활용

BlockHound도 완벽하진 않다. 특정 라이브러리가 의도적으로 블로킹 호출을 해야 한다면? 그땐 그걸 명시적으로 허용해야 한다. 커스텀 설정으로 탐지 규칙을 조정할 수 있다.

```java
@BeforeAll
static void setup() {
    BlockHound.install(builder -> builder
        // 특정 클래스/메서드의 블로킹 허용 (레거시 라이브러리 등)
        .allowBlockingCallsInside(
            "com.example.legacy.LegacyService", "initialize")
        // 특정 스레드 이름 패턴 제외
        .nonBlockingThreadPredicate(current ->
            current.or(t -> t.getName().startsWith("blocking-worker")))
    );
}
```

StepVerifier와 BlockHound를 함께 쓰면 블로킹 호출을 자동으로 잡아내는 테스트를 만들 수 있다.

```java
@SpringBootTest
class ProductServiceBlockingTest {

    @BeforeAll
    static void setup() {
        BlockHound.install();
    }

    @Autowired
    private ProductService productService;

    @Test
    void findById_논블로킹_검증() {
        StepVerifier.create(
            Mono.defer(() -> productService.findById("test-id"))
                .subscribeOn(Schedulers.parallel())  // 논블로킹 스레드에서 실행
        )
        .expectNextCount(1)
        .verifyComplete();
        // 블로킹 호출이 있으면 ReactorBlockHoundIntegration 예외 발생
    }
}
```

> **참고**: BlockHound는 프로덕션에서는 절대 켜면 안 된다. JVM Agent 방식이라 오버헤드가 크고, 초기화 같은 정당한 블로킹 호출에서도 예외를 던진다. 개발이나 테스트, 스테이징 환경에만 켜두자.

---

## 19.6 부하 테스트 (Gatling, k6)

지금까지 최적화 기법들을 봤는데, 이게 실제로 효과가 있는지 어떻게 알까? 실제 부하 조건에서 테스트해야 한다.

### 19.6.1 Gatling 부하 테스트

Gatling은 부하 테스트를 위한 강력한 도구다. Scala로 만들어졌지만, Java DSL을 써서 테스트 시나리오를 작성할 수도 있다.

```groovy
plugins {
    id 'io.gatling.gradle' version '3.11.5.2'
}

dependencies {
    gatlingImplementation 'io.gatling.highcharts:gatling-charts-highcharts:3.11.5'
}
```

```java
// src/gatling/java/simulations/ProductApiSimulation.java
public class ProductApiSimulation extends Simulation {

    HttpProtocolBuilder httpProtocol = http
        .baseUrl("http://localhost:8080")
        .acceptHeader("application/json")
        .contentTypeHeader("application/json");

    ScenarioBuilder listProducts = scenario("제품 목록 조회")
        .exec(
            http("GET /api/products")
                .get("/api/products")
                .queryParam("page", "0")
                .queryParam("size", "20")
                .check(status().is(200))
        )
        .pause(Duration.ofMillis(100), Duration.ofMillis(500));

    ScenarioBuilder createProduct = scenario("제품 등록")
        .exec(
            http("POST /api/products")
                .post("/api/products")
                .body(StringBody("""
                    {"name":"테스트 상품","price":10000,"category":"electronics"}
                    """))
                .check(status().is(201))
        )
        .pause(Duration.ofMillis(200), Duration.ofMillis(1000));

    {
        setUp(
            listProducts.injectOpen(
                rampUsersPerSec(10).to(200).during(Duration.ofMinutes(2)),
                constantUsersPerSec(200).during(Duration.ofMinutes(3)),
                rampUsersPerSec(200).to(500).during(Duration.ofMinutes(2)),
                constantUsersPerSec(500).during(Duration.ofMinutes(3))
            ),
            createProduct.injectOpen(
                constantUsersPerSec(20).during(Duration.ofMinutes(10))
            )
        ).protocols(httpProtocol)
         .assertions(
             global().responseTime().percentile3().lt(500),
             global().successfulRequests().percent().gt(99.0)
         );
    }
}
```

### 19.6.2 k6 부하 테스트

k6는 좀 더 현대적이고 간단한 부하 테스트 도구다. Go로 만들어졌고, JavaScript로 테스트를 작성하면 된다. CLI로 실행하는 것도 정말 간단하다.

```javascript
// load-test.js
import http from 'k6/http';
import { check, sleep } from 'k6';
import { Rate, Trend } from 'k6/metrics';

const errorRate = new Rate('errors');
const productListDuration = new Trend('product_list_duration');

export const options = {
    stages: [
        { duration: '1m', target: 50 },
        { duration: '3m', target: 200 },
        { duration: '2m', target: 500 },
        { duration: '3m', target: 500 },
        { duration: '1m', target: 0 },
    ],
    thresholds: {
        http_req_duration: ['p(95)<500', 'p(99)<1000'],
        errors: ['rate<0.01'],
    },
};

const BASE_URL = 'http://localhost:8080';

export default function () {
    const listRes = http.get(`${BASE_URL}/api/products?page=0&size=20`);
    productListDuration.add(listRes.timings.duration);

    check(listRes, {
        'status is 200': (r) => r.status === 200,
        'response time < 500ms': (r) => r.timings.duration < 500,
    });
    errorRate.add(listRes.status !== 200);
    sleep(Math.random() * 0.5);

    const createRes = http.post(
        `${BASE_URL}/api/products`,
        JSON.stringify({ name: `상품 ${Date.now()}`, price: 10000, category: 'test' }),
        { headers: { 'Content-Type': 'application/json' } }
    );
    check(createRes, { 'created': (r) => r.status === 201 });
    sleep(Math.random() * 1);
}
```

```bash
# 실행
k6 run load-test.js

# Grafana 연동 (InfluxDB로 메트릭 전송)
k6 run --out influxdb=http://localhost:8086/k6 load-test.js
```

### 19.6.3 MVC vs WebFlux 성능 비교

그럼 이제 흔한 질문을 답해보자: Spring MVC와 Spring WebFlux는 성능이 얼마나 다를까? 동일한 비즈니스 로직으로 비교한 결과를 정리해봤다. 물론 실제 수치는 하드웨어와 로직의 특성에 따라 달라진다.

| 지표 | Spring MVC | Spring WebFlux | 비고 |
|------|-----------|---------------|------|
| **동시 사용자 100명** | | | |
| 처리량 | ~5,000 req/sec | ~6,000 req/sec | 유사 |
| p95 지연시간 | ~20ms | ~18ms | 유사 |
| 스레드 수 | ~200 | ~8 | WebFlux 압도적 |
| 메모리 사용량 | ~512MB | ~256MB | WebFlux 절반 |
| **동시 사용자 1,000명** | | | |
| 처리량 | ~4,500 req/sec | ~9,000 req/sec | WebFlux 2배 |
| p95 지연시간 | ~250ms | ~50ms | WebFlux 5배 |
| 스레드 수 | ~1,000+ | ~8 | WebFlux 압도적 |
| **동시 사용자 5,000명** | | | |
| 처리량 | ~3,000 req/sec | ~8,500 req/sec | WebFlux 3배 |
| p95 지연시간 | ~2,000ms+ | ~120ms | WebFlux 16배 |
| 에러율 | ~5% | ~0.1% | MVC 커넥션 거부 |

> **핵심 포인트**: 동시 사용자가 적을 때는 둘 다 엇비슷하다. WebFlux의 진정한 가치는 **높은 동시성**이 필요한 상황에서 드러난다. 외부 API 호출이 많거나 느린 DB 쿼리가 있는 애플리케이션일수록 WebFlux가 훨씬 유리하다.

### 19.6.4 부하 테스트 결과 분석 체크리스트

부하 테스트 결과를 볼 때 뭘 봐야 할까? 이런 체크리스트를 참고하면 된다.

| 점검 항목 | 정상 기준 | 이상 시 대응 |
|----------|----------|-------------|
| **p95 응답 시간** | SLA 목표 이내 | 병목 구간 프로파일링 |
| **에러율** | < 0.1% | 에러 로그 분석, 타임아웃 조정 |
| **CPU 사용률** | < 80% | 스케줄러 오프로드, 알고리즘 최적화 |
| **메모리 사용률** | GC 오버헤드 < 5% | 힙 크기 조정, 객체 풀링 |
| **커넥션 풀 대기** | waitQueue = 0 | 풀 크기 증가, 쿼리 최적화 |
| **이벤트 루프 CPU** | 각 스레드 < 70% | 블로킹 코드 제거, 연산 오프로드 |

### 19.6.5 성능 최적화 사이클

성능 최적화는 한 번 끝나는 게 아니다. 계속 반복되는 과정이다.

```
1. 측정 (Baseline)       <- Gatling/k6로 현재 성능 측정
2. 분석 (Bottleneck)     <- 프로파일링, 메트릭, 로그 분석
3. 최적화 (Fix)          <- 커넥션 풀, 캐시, 블로킹 제거, 쿼리 최적화
4. 검증 (Verify)         <- 동일 조건에서 재측정, 비교
5. 반복 (Iterate)        <- 다음 병목 지점으로 이동
```

한 번에 여러 개를 손보면 뭐가 효과가 있었는지 알 수 없다. **한 번에 하나씩만 바꾸고 측정하자**. 이게 원칙이다.

---

## 요약

이 장에서 다룬 내용을 정리해보면 다음과 같다.

| 주제 | 핵심 내용 |
|------|----------|
| **성능 측정** | 처리량/지연시간/리소스 3축 측정, Micrometer 메트릭, JMH 마이크로벤치마크, async-profiler/JFR 프로파일링 |
| **MongoDB 커넥션 풀** | `MongoClientSettings`로 풀 크기/타임아웃 설정, 커넥션 풀 메트릭 모니터링, 계층별 타임아웃 전략 |
| **Netty 이벤트 루프** | `LoopResources`로 스레드 수 조정, Epoll/KQueue 네이티브 Transport, 블로킹 작업 스케줄러 오프로드 |
| **캐싱 전략** | Caffeine 로컬 캐시, Reactive Redis 분산 캐시, L1/L2 멀티 레벨 캐시 구조 |
| **BlockHound** | 블로킹 호출 런타임 탐지, 흔한 블로킹 패턴과 수정법, StepVerifier 테스트 통합 |
| **부하 테스트** | Gatling/k6 스크립트 작성, MVC vs WebFlux 성능 비교, 결과 분석 체크리스트, 최적화 사이클 |

성능 최적화에서 가장 중요한 건 뭘까? 결국 **측정 -> 분석 -> 최적화 -> 검증**을 계속 반복하는 것이다. 감에 의존하지 말고, 항상 데이터를 기반으로 판단해야 한다. 그게 성공하는 최적화의 비결이다.

다음 장에서는 애플리케이션을 Docker 컨테이너로 만들고, Kubernetes에 배포하고, CI/CD 파이프라인을 구성하는 방법을 다룬다.
