# Chapter 20. 컨테이너화와 배포

개발을 마치고 실전 운영 환경으로 나가려면 어떻게 해야 할까? 그 답이 **컨테이너(Container)**다. Docker와 Kubernetes는 이제 거의 표준이 되었다. Docker를 쓰면 애플리케이션과 실행 환경을 하나의 이미지로 묶어 어디서든 동일하게 실행할 수 있고, Kubernetes로는 수십 개의 컨테이너를 마치 한 대의 머신인 것처럼 관리할 수 있다.

이번 장에서는 실제 프로젝트에서 어떻게 Spring Boot WebFlux + MongoDB 애플리케이션을 컨테이너화하고 운영 환경으로 보내는지 살펴본다. Docker 이미지 빌드부터 Docker Compose를 이용한 로컬 환경 구성, Kubernetes 배포, MongoDB Atlas 연동, GitHub Actions로 자동화된 CI/CD 파이프라인 구축, 그리고 GraalVM Native Image까지 단계별로 진행해보자.

---

## 20.1 Docker 이미지 빌드 (Jib, Buildpacks)

Docker 이미지를 만드는 방법은 여러 가지가 있다. 전통적인 Dockerfile부터 시작해서 요즘 핫한 도구들까지 선택지가 많다. Google Jib과 Spring Boot의 Cloud Native Buildpacks가 가장 인기 있는 방식이고, 각각 장단점이 있다.

### 20.1.1 Jib을 활용한 Docker 이미지 빌드

Jib은 Google이 만든 도구인데, 사실 꽤 편리하다. Docker 데몬을 설치하고 실행할 필요가 없다는 게 큰 장점이다. **Docker 데몬 없이도** 빌드하고 바로 레지스트리에 올릴 수 있다. `build.gradle`에 플러그인을 추가하기만 하면 된다.

```groovy
plugins {
    id 'org.springframework.boot' version '3.4.1'
    id 'io.spring.dependency-management' version '1.1.7'
    id 'com.google.cloud.tools.jib' version '3.4.4'
    id 'java'
}

jib {
    from {
        image = 'eclipse-temurin:21-jre'
        platforms {
            platform {
                architecture = 'amd64'
                os = 'linux'
            }
            platform {
                architecture = 'arm64'
                os = 'linux'
            }
        }
    }
    to {
        image = 'ghcr.io/myorg/webflux-app'
        tags = [project.version, 'latest']
        auth {
            username = System.getenv('REGISTRY_USERNAME')
            password = System.getenv('REGISTRY_PASSWORD')
        }
    }
    container {
        jvmFlags = [
            '-XX:+UseZGC',
            '-XX:MaxRAMPercentage=75.0',
            '-Djava.security.egd=file:/dev/./urandom',
            '-Dspring.profiles.active=prod'
        ]
        ports = ['8080']
        creationTime = 'USE_CURRENT_TIMESTAMP'
        user = '1000:1000'
    }
}
```

상황에 따라 사용할 수 있는 명령어가 세 가지다.

```bash
# Docker 데몬 없이 레지스트리에 직접 푸시
./gradlew jib

# 로컬 Docker 데몬에 이미지 빌드
./gradlew jibDockerBuild

# tar 파일로 이미지 내보내기
./gradlew jibBuildTar
```

> **Tip**: Jib은 애플리케이션을 `classes`, `resources`, `dependencies`, `snapshot-dependencies`의 네 레이어로 자동 분리한다. 소스 코드만 변경하면 `classes` 레이어만 다시 빌드되므로 CI/CD에서 빌드 시간을 크게 단축할 수 있다.

### 20.1.2 Cloud Native Buildpacks

Spring Boot 3.x부터는 Cloud Native Buildpacks를 바로 사용할 수 있다. 추가 플러그인 설치 없이 기본 Gradle 명령어로 OCI 이미지를 빌드하면 되니까 편하다.

```groovy
bootBuildImage {
    imageName = "ghcr.io/myorg/webflux-app:${project.version}"
    environment = [
        'BP_JVM_VERSION': '21',
        'BP_JVM_TYPE': 'JRE',
        'BPE_JAVA_TOOL_OPTIONS': '-XX:+UseZGC -XX:MaxRAMPercentage=75.0'
    ]
    docker {
        publishRegistry {
            username = System.getenv('REGISTRY_USERNAME')
            password = System.getenv('REGISTRY_PASSWORD')
            url = 'https://ghcr.io'
        }
    }
}
```

```bash
./gradlew bootBuildImage
```

### 20.1.3 빌드 방식 비교

세 가지 방식의 특징을 정리하면 아래와 같다.

| 항목 | Dockerfile | Jib | Buildpacks |
|------|-----------|-----|------------|
| Docker 데몬 필요 | O | X | O |
| Dockerfile 필요 | O | X | X |
| 빌드 속도 | 보통 | 빠름 | 느림 |
| 레이어 최적화 | 수동 | 자동 | 자동 |
| 멀티 아키텍처 | 수동 설정 | 선언적 | 제한적 |
| CI/CD 친화성 | 보통 | 높음 | 높음 |

필자의 경험상, Buildpacks는 초기 빌드가 상당히 오래 걸린다. 하지만 캐시가 쌓인 후에는 빌드가 빨라진다. CI/CD 환경에서 Docker 데몬을 설정하기 어렵다면 Jib을 쓰는 게 가장 무난하다.

---

## 20.2 Docker Compose로 전체 스택 구성

이제 로컬에서 전체 환경을 한 번에 띄워보자. Docker Compose를 쓰면 Spring Boot 앱, MongoDB, Prometheus, Grafana를 한 번의 명령어로 구성할 수 있다.

### 20.2.1 Docker Compose 구성 파일

```yaml
# docker/docker-compose.yml
version: '3.8'
services:
  app:
    build:
      context: ..
      dockerfile: Dockerfile
    container_name: webflux-app
    ports:
      - "8080:8080"
    environment:
      SPRING_PROFILES_ACTIVE: docker
      SPRING_DATA_MONGODB_URI: mongodb://appuser:apppass@mongodb:27017/webfluxdb?authSource=admin
      JAVA_TOOL_OPTIONS: >-
        -XX:+UseZGC
        -XX:MaxRAMPercentage=75.0
    depends_on:
      mongodb:
        condition: service_healthy
    networks:
      - app-network
    deploy:
      resources:
        limits:
          memory: 512M
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/actuator/health"]
      interval: 15s
      timeout: 5s
      retries: 5
      start_period: 30s
  mongodb:
    image: mongo:7.0
    container_name: mongodb
    ports:
      - "27017:27017"
    environment:
      MONGO_INITDB_ROOT_USERNAME: appuser
      MONGO_INITDB_ROOT_PASSWORD: apppass
      MONGO_INITDB_DATABASE: webfluxdb
    volumes:
      - mongodb-data:/data/db
      - ./mongo/init-mongo.js:/docker-entrypoint-initdb.d/init-mongo.js:ro
    networks:
      - app-network
    healthcheck:
      test: ["CMD", "mongosh", "--eval", "db.adminCommand('ping')"]
      interval: 10s
      timeout: 5s
      retries: 5
  prometheus:
    image: prom/prometheus:v2.51.0
    container_name: prometheus
    ports:
      - "9090:9090"
    volumes:
      - ./prometheus/prometheus.yml:/etc/prometheus/prometheus.yml:ro
      - prometheus-data:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.retention.time=7d'
    networks:
      - app-network
  grafana:
    image: grafana/grafana:10.4.0
    container_name: grafana
    ports:
      - "3000:3000"
    environment:
      GF_SECURITY_ADMIN_USER: admin
      GF_SECURITY_ADMIN_PASSWORD: admin
    volumes:
      - grafana-data:/var/lib/grafana
      - ./grafana/provisioning:/etc/grafana/provisioning:ro
    networks:
      - app-network
    depends_on:
      - prometheus
volumes:
  mongodb-data:
  prometheus-data:
  grafana-data:
networks:
  app-network:
    driver: bridge
```

### 20.2.2 Prometheus 설정과 MongoDB 초기화

```yaml
# docker/prometheus/prometheus.yml
global:
  scrape_interval: 15s
scrape_configs:
  - job_name: 'webflux-app'
    metrics_path: '/actuator/prometheus'
    scrape_interval: 5s
    static_configs:
      - targets: ['app:8080']
```

```javascript
// docker/mongo/init-mongo.js
db = db.getSiblingDB('webfluxdb');
db.createCollection('products');
db.products.createIndex({ "name": 1 }, { unique: true });
db.products.createIndex({ "category": 1, "price": 1 });
```

### 20.2.3 Docker 프로파일용 애플리케이션 설정

```yaml
# src/main/resources/application-docker.yml
spring:
  data:
    mongodb:
      uri: ${SPRING_DATA_MONGODB_URI:mongodb://localhost:27017/webfluxdb}

management:
  endpoints:
    web:
      exposure:
        include: health,info,prometheus,metrics
  endpoint:
    health:
      show-details: always
      probes:
        enabled: true
server:
  port: 8080
  netty:
    connection-timeout: 5s
```

```bash
# 전체 스택 시작
docker compose -f docker/docker-compose.yml up -d

# 로그 확인
docker compose -f docker/docker-compose.yml logs -f app

# 전체 스택 종료 및 볼륨 삭제
docker compose -f docker/docker-compose.yml down -v
```

실제로는 이 정도면 로컬에서 프로덕션과 거의 같은 환경을 구성할 수 있다.

---

## 20.3 Kubernetes 배포 기초

프로덕션으로 나가려면 보통 Kubernetes(K8s)를 쓴다. 수십, 수백 개의 컨테이너를 관리해야 할 때 정말 강력하다.

### 20.3.1 ConfigMap과 Secret

```yaml
# k8s/base/configmap.yml
apiVersion: v1
kind: ConfigMap
metadata:
  name: webflux-app-config
  namespace: webflux-app
data:
  SPRING_PROFILES_ACTIVE: "k8s"
  JAVA_TOOL_OPTIONS: >-
    -XX:+UseZGC
    -XX:MaxRAMPercentage=75.0
    -XX:+ExitOnOutOfMemoryError
  MANAGEMENT_SERVER_PORT: "8081"
```

```yaml
# k8s/base/secret.yml
apiVersion: v1
kind: Secret
metadata:
  name: webflux-app-secret
  namespace: webflux-app
type: Opaque
stringData:
  SPRING_DATA_MONGODB_URI: "mongodb+srv://appuser:securepass@cluster0.example.mongodb.net/webfluxdb?retryWrites=true&w=majority"
```

> **주의**: Secret을 Git에 평문으로 커밋하면 안 된다. 프로덕션에서는 **Sealed Secrets**, **External Secrets Operator**, 또는 **HashiCorp Vault**를 사용하여 시크릿을 관리한다.

### 20.3.2 Deployment

```yaml
# k8s/base/deployment.yml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: webflux-app
  namespace: webflux-app
spec:
  replicas: 2
  selector:
    matchLabels:
      app.kubernetes.io/name: webflux-app
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxUnavailable: 0
      maxSurge: 1
  template:
    metadata:
      labels:
        app.kubernetes.io/name: webflux-app
      annotations:
        prometheus.io/scrape: "true"
        prometheus.io/port: "8081"
        prometheus.io/path: "/actuator/prometheus"
    spec:
      terminationGracePeriodSeconds: 30
      containers:
        - name: webflux-app
          image: ghcr.io/myorg/webflux-app:1.0.0
          ports:
            - name: http
              containerPort: 8080
            - name: management
              containerPort: 8081
          envFrom:
            - configMapRef:
                name: webflux-app-config
            - secretRef:
                name: webflux-app-secret
          resources:
            requests:
              memory: "256Mi"
              cpu: "250m"
            limits:
              memory: "512Mi"
              cpu: "1000m"
          readinessProbe:
            httpGet:
              path: /actuator/health/readiness
              port: management
            initialDelaySeconds: 10
            periodSeconds: 5
          livenessProbe:
            httpGet:
              path: /actuator/health/liveness
              port: management
            initialDelaySeconds: 30
            periodSeconds: 10
          startupProbe:
            httpGet:
              path: /actuator/health/liveness
              port: management
            initialDelaySeconds: 5
            periodSeconds: 5
            failureThreshold: 20
          lifecycle:
            preStop:
              exec:
                command: ["sh", "-c", "sleep 5"]
```

여기서 중요한 게 K8s 프로브와 Spring Boot Actuator의 관계다. 각 프로브가 어떤 엔드포인트를 호출하는지 잘 이해해야 한다.

| K8s 프로브 | Actuator 엔드포인트 | 역할 |
|-----------|-------------------|------|
| `startupProbe` | `/actuator/health/liveness` | 애플리케이션 시작 완료 확인 |
| `readinessProbe` | `/actuator/health/readiness` | 트래픽 수신 준비 확인 |
| `livenessProbe` | `/actuator/health/liveness` | 프로세스 정상 동작 확인 |

그리고 `preStop` 훅의 `sleep 5`가 중요한데, 이건 K8s가 Service 엔드포인트 목록에서 파드를 제거하는 동안 추가 시간을 주는 거다. 이렇게 해야 **정상 종료(Graceful Shutdown)** 중에 요청을 잃지 않는다.

### 20.3.3 Service와 HPA

```yaml
# k8s/base/service.yml
apiVersion: v1
kind: Service
metadata:
  name: webflux-app
  namespace: webflux-app
spec:
  type: ClusterIP
  ports:
    - name: http
      port: 80
      targetPort: http
  selector:
    app.kubernetes.io/name: webflux-app
```

```yaml
# k8s/base/hpa.yml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: webflux-app
  namespace: webflux-app
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: webflux-app
  minReplicas: 2
  maxReplicas: 10
  metrics:
    - type: Resource
      resource:
        name: cpu
        target:
          type: Utilization
          averageUtilization: 70
  behavior:
    scaleDown:
      stabilizationWindowSeconds: 300
```

여기서 한 가지 주의할 점이 있다. 필자의 경험상 리액티브 애플리케이션은 CPU 사용률이 낮아 보이는데도 높은 처리량을 낸다. 따라서 CPU 기반 HPA만으로는 정확한 스케일링 시점을 판단하기 어렵다. 커스텀 메트릭(요청 큐 크기, p99 지연시간 등)을 함께 활용하는 게 좋다.

---

## 20.4 MongoDB Atlas 클라우드 연동

프로덕션에서 MongoDB를 직접 관리하는 건 번거롭다. **MongoDB Atlas**라는 관리형 서비스를 쓰면 훨씬 편하다. 필자의 경험상 자동 백업, 패치 관리, 고가용성 설정 등이 모두 자동으로 되니까 개발팀이 코드에 집중할 수 있다.

### 20.4.1 Atlas 연결 설정

```yaml
# src/main/resources/application-prod.yml
spring:
  data:
    mongodb:
      uri: mongodb+srv://${MONGO_USERNAME}:${MONGO_PASSWORD}@cluster0.abc123.mongodb.net/${MONGO_DATABASE}?retryWrites=true&w=majority&maxPoolSize=50&minPoolSize=5&connectTimeoutMS=10000&serverSelectionTimeoutMS=10000
```

| 옵션 | 권장값 | 설명 |
|------|-------|------|
| `retryWrites` | true | 일시적 네트워크 오류 시 쓰기 자동 재시도 |
| `w` | majority | 과반수 노드 쓰기 확인 |
| `maxPoolSize` | 50 | 최대 커넥션 풀 크기 |
| `minPoolSize` | 5 | 최소 유지 커넥션 수 |
| `connectTimeoutMS` | 10000 | 연결 타임아웃 |

### 20.4.2 Java 설정과 헬스 체크

```java
@Configuration
public class MongoAtlasConfig {

    @Bean
    public MongoClientSettings mongoClientSettings() {
        return MongoClientSettings.builder()
            .applyConnectionString(
                new ConnectionString(System.getenv("SPRING_DATA_MONGODB_URI"))
            )
            .applyToSslSettings(ssl -> ssl.enabled(true))
            .applyToConnectionPoolSettings(pool -> pool
                .maxSize(50)
                .minSize(5)
                .maxConnectionIdleTime(60, TimeUnit.SECONDS)
                .maxWaitTime(10, TimeUnit.SECONDS)
            )
            .retryWrites(true)
            .retryReads(true)
            .build();
    }
}
```

```java
@Component
@RequiredArgsConstructor
public class MongoAtlasHealthIndicator implements ReactiveHealthIndicator {

    private final ReactiveMongoTemplate mongoTemplate;

    @Override
    public Mono<Health> health() {
        return mongoTemplate.executeCommand(new org.bson.Document("ping", 1))
            .map(result -> Health.up()
                .withDetail("database", "MongoDB Atlas")
                .build())
            .onErrorResume(ex -> Mono.just(
                Health.down()
                    .withDetail("error", ex.getMessage())
                    .build()))
            .timeout(Duration.ofSeconds(5))
            .onErrorReturn(Health.down()
                .withDetail("error", "Health check timeout")
                .build());
    }
}
```

### 20.4.3 K8s에서 Atlas 연결 시 고려사항

K8s에서 Atlas로 연결할 때 생각해야 할 부분들이 있다.

1. **고정 Egress IP**: K8s 클러스터의 모든 아웃바운드 트래픽이 고정 IP를 사용하도록 NAT Gateway를 설정하고, 그 IP를 Atlas IP Access List에 등록한다.
2. **VPC Peering / Private Link**: 보안이 중요한 프로덕션 환경이라면 Atlas의 VPC Peering이나 AWS PrivateLink를 설정하자.
3. **DNS 해석**: `mongodb+srv://` URI는 DNS SRV 레코드를 사용한다. K8s 클러스터의 DNS가 외부 DNS를 제대로 해석할 수 있어야 한다.

한 가지 더, Atlas의 무료 등급(M0)은 VPC Peering과 Private Link를 지원하지 않는다. 프로덕션에서는 최소 M10 이상을 써야 한다.

---

## 20.5 CI/CD 파이프라인 구성 (GitHub Actions)

이제 자동화 차례다. GitHub Actions를 쓰면 코드 푸시부터 테스트, 빌드, K8s 배포까지 자동으로 진행된다. 직접 배포할 필요 없이 git push 하나면 된다.

### 20.5.1 CI/CD 워크플로우

```yaml
# .github/workflows/ci.yml
name: CI/CD Pipeline

on:
  push:
    branches: [main, develop]
  pull_request:
    branches: [main]

permissions:
  contents: read
  packages: write

env:
  JAVA_VERSION: '21'
  REGISTRY: ghcr.io
  IMAGE_NAME: ${{ github.repository }}

jobs:
  test:
    name: Build & Test
    runs-on: ubuntu-latest
    services:
      mongodb:
        image: mongo:7.0
        ports:
          - 27017:27017
        env:
          MONGO_INITDB_ROOT_USERNAME: testuser
          MONGO_INITDB_ROOT_PASSWORD: testpass
        options: >-
          --health-cmd "mongosh --eval 'db.adminCommand({ping:1})'"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-java@v4
        with:
          java-version: ${{ env.JAVA_VERSION }}
          distribution: 'temurin'
      - uses: gradle/actions/setup-gradle@v4
      - name: Run Tests
        env:
          SPRING_DATA_MONGODB_URI: mongodb://testuser:testpass@localhost:27017/testdb?authSource=admin
        run: ./gradlew test

  build-image:
    name: Build & Push Image
    needs: test
    runs-on: ubuntu-latest
    if: github.event_name == 'push' && github.ref == 'refs/heads/main'
    outputs:
      image-tag: ${{ steps.meta.outputs.version }}
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-java@v4
        with:
          java-version: ${{ env.JAVA_VERSION }}
          distribution: 'temurin'
      - uses: gradle/actions/setup-gradle@v4
      - uses: docker/login-action@v3
        with:
          registry: ${{ env.REGISTRY }}
          username: ${{ github.actor }}
          password: ${{ secrets.GITHUB_TOKEN }}
      - name: Extract metadata
        id: meta
        uses: docker/metadata-action@v5
        with:
          images: ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}
          tags: |
            type=sha,prefix=
            type=ref,event=branch
      - name: Build and Push with Jib
        run: |
          ./gradlew jib \
            -Djib.to.image=${{ env.REGISTRY }}/${{ env.IMAGE_NAME }} \
            -Djib.to.tags=${{ steps.meta.outputs.version }}
        env:
          REGISTRY_USERNAME: ${{ github.actor }}
          REGISTRY_PASSWORD: ${{ secrets.GITHUB_TOKEN }}

  deploy:
    name: Deploy to Kubernetes
    needs: build-image
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/main'
    environment: production
    steps:
      - uses: actions/checkout@v4
      - uses: azure/setup-kubectl@v4
      - name: Set Kubeconfig
        run: |
          mkdir -p $HOME/.kube
          echo "${{ secrets.KUBE_CONFIG }}" | base64 -d > $HOME/.kube/config
      - name: Deploy
        run: |
          kubectl apply -k k8s/overlays/prod/
          kubectl -n webflux-app rollout status deployment/webflux-app --timeout=300s
```

### 20.5.2 브랜치 전략과 시크릿 관리

브랜치별로 다른 동작을 하도록 설정할 수 있다.

| 브랜치 | 트리거 | 수행 작업 |
|--------|-------|----------|
| `feature/*` | PR 생성 | 빌드, 테스트 |
| `develop` | Push | 빌드, 테스트, 스테이징 배포 |
| `main` | Push | 빌드, 테스트, 이미지 빌드, 프로덕션 배포 |

필요한 GitHub Secrets:

| 시크릿 이름 | 설명 |
|------------|------|
| `GITHUB_TOKEN` | 자동 제공 (GHCR 인증용) |
| `KUBE_CONFIG` | K8s kubeconfig (Base64 인코딩) |
| `MONGO_URI` | MongoDB Atlas 연결 문자열 |

팁으로, `environment: production` 설정을 쓰면 프로덕션 배포 전에 수동 승인(Manual Approval) 단계를 추가할 수 있다. 이렇게 하면 실수로 잘못된 코드가 배포되는 실수를 줄일 수 있다.

---

## 20.6 GraalVM Native Image 빌드

마지막으로 GraalVM Native Image를 살펴보자. 이건 Java를 컴파일 시점에 네이티브 코드로 변환해버린다. 덕분에 시작 시간이 몇 초에서 수백 밀리초로 줄어들고, 메모리도 훨씬 적게 쓴다. 서버리스 환경이나 마이크로서비스 아키텍처에 정말 좋다.

### 20.6.1 JVM vs Native Image 비교

| 항목 | JVM | Native Image |
|------|-----|-------------|
| 시작 시간 | 2~5초 | 50~200ms |
| 메모리 사용 | 256~512MB | 64~128MB |
| 최대 처리량 | 높음 (JIT 최적화) | 보통 (AOT 제한) |
| 빌드 시간 | 짧음 | 김 (수 분) |
| 리플렉션 지원 | 완전 | 설정 필요 |

### 20.6.2 Gradle 빌드 설정

```groovy
plugins {
    id 'org.springframework.boot' version '3.4.1'
    id 'io.spring.dependency-management' version '1.1.7'
    id 'org.graalvm.buildtools.native' version '0.10.4'
    id 'java'
}

graalvmNative {
    binaries {
        main {
            buildArgs.addAll([
                '--initialize-at-build-time',
                '-H:+ReportExceptionStackTraces'
            ])
            javaLauncher = javaToolchains.launcherFor {
                languageVersion = JavaLanguageVersion.of(21)
                vendor = JvmVendorSpec.GRAAL_VM
            }
        }
    }
    metadataRepository {
        enabled = true
    }
}
```

### 20.6.3 리플렉션 힌트 설정

Spring Boot 3.x는 대부분의 리플렉션 힌트를 자동으로 생성해준다. 다행히 개발자가 직접 설정할 필요가 거의 없다. 다만 동적으로 로드되는 특수한 클래스는 수동으로 등록해야 한다.

```java
@Configuration
@ImportRuntimeHints(NativeHintsRegistrar.class)
public class NativeImageConfig {
}

public class NativeHintsRegistrar implements RuntimeHintsRegistrar {

    @Override
    public void registerHints(RuntimeHints hints, ClassLoader classLoader) {
        hints.reflection()
            .registerType(Product.class, MemberCategory.values())
            .registerType(Order.class, MemberCategory.values());

        hints.resources()
            .registerPattern("application*.yml");
    }
}
```

### 20.6.4 Native Image Docker 빌드

GraalVM을 로컬에 설치할 필요는 없다. Buildpacks를 쓰면 Docker 컨테이너 안에서 자동으로 빌드된다.

```groovy
bootBuildImage {
    imageName = "ghcr.io/myorg/webflux-app-native:${project.version}"
    environment = [
        'BP_NATIVE_IMAGE': 'true',
        'BP_JVM_VERSION': '21'
    ]
}
```

혹은 멀티 스테이지 Dockerfile로 직접 빌드할 수도 있다.

```dockerfile
FROM ghcr.io/graalvm/native-image-community:21 AS builder
WORKDIR /app
COPY gradle/ gradle/
COPY gradlew build.gradle settings.gradle ./
COPY src/ src/
RUN ./gradlew nativeCompile --no-daemon

FROM debian:bookworm-slim
WORKDIR /app
RUN groupadd -r appuser && useradd -r -g appuser appuser
COPY --from=builder /app/build/native/nativeCompile/webflux-app ./
USER appuser
EXPOSE 8080
ENTRYPOINT ["./webflux-app"]
```

### 20.6.5 주의사항

실제로 네이티브 이미지를 빌드할 때 주의할 점들이 있다.

1. **빌드 리소스**: 네이티브 이미지 컴파일은 상당히 무겁다. 최소 8GB 메모리가 필요하고 5~10분 이상 걸린다. CI/CD에서 대규모 프로젝트는 GitHub Actions의 Larger Runner를 사용하는 게 좋다.
2. **프로파일 결정 시점**: 네이티브 이미지는 빌드할 때 프로파일이 확정된다. 런타임에 바꿀 수 없다는 뜻이다. 나중에 런타임 변경이 필요하면 AOT 처리 시 명시적으로 설정해야 한다.

```bash
./gradlew nativeCompile -Pspring.profiles.active=prod
```

3. **서드파티 호환성**: 모든 라이브러리가 네이티브 이미지를 지원하진 않는다. 사용하는 라이브러리가 지원되는지 [GraalVM Reachability Metadata Repository](https://github.com/oracle/graalvm-reachability-metadata)에서 확인하자.
4. **GitHub Actions 빌드**: 보통 Git 태그를 푸시할 때 네이티브 이미지 빌드를 트리거하는 방식으로 구성한다.

```yaml
# .github/workflows/native-build.yml
name: Native Image Build
on:
  push:
    tags: ['v*']
jobs:
  native-build:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - uses: graalvm/setup-graalvm@v1
        with:
          java-version: '21'
          distribution: 'graalvm-community'
      - uses: gradle/actions/setup-gradle@v4
      - run: ./gradlew nativeCompile
      - run: |
          docker build -f Dockerfile.native \
            -t ghcr.io/${{ github.repository }}-native:${{ github.ref_name }} .
          echo "${{ secrets.GITHUB_TOKEN }}" | docker login ghcr.io -u ${{ github.actor }} --password-stdin
          docker push ghcr.io/${{ github.repository }}-native:${{ github.ref_name }}
```

---

## 요약

지금까지 배포의 전체 과정을 다뤘다.

| 주제 | 핵심 도구 | 권장 사항 |
|------|----------|----------|
| Docker 빌드 | Jib, Buildpacks | CI/CD에서는 Jib 권장 |
| 로컬 스택 | Docker Compose | 헬스 체크와 의존성 순서 필수 |
| K8s 배포 | Deployment, HPA | 3종 프로브와 preStop 훅 설정 |
| Atlas 연동 | SRV 연결 | VPC Peering으로 보안 강화 |
| CI/CD | GitHub Actions | 브랜치 전략과 시크릿 관리 |
| Native Image | GraalVM, AOT | 서버리스 환경에 적합 |

컨테이너화와 CI/CD를 제대로 구축해놓으면, 이후 배포는 거의 자동화된다. 개발팀은 코드만 푸시하면 되고, 나머지는 파이프라인이 알아서 처리한다. 다음 장에서는 이런 환경에서 문제가 생겼을 때 **장애 대응과 트러블슈팅**을 어떻게 하는지 다룬다.